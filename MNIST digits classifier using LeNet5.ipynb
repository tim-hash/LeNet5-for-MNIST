{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LeNet5 for MNIST digits classification\n\nIn this notebook I will use a LeNet5 implementation using tensorflow keras API for the problem of handwritten digits classifcation. This is tribute to he man who created the data set and the performance of this CNN is very satisfying for not an acceptable number of parameters."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom scipy import ndimage\nimport matplotlib.pyplot as plt\nimport time\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":60,"outputs":[{"output_type":"stream","text":"/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n/kaggle/input/digit-recognizer/sample_submission.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Loading the data\n\nI start by loading the data onto the notebook by using Pandas as the format is CSV.**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then I quickly visualize the five first rows of the data using the head() method from pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Convert the panda dataframe into a numpy array\n\nConvert the panda dataframe to a numpy array. Shuffle the data using a sklearn tool and show a random example image."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_as_np = data.to_numpy()\nlabels = data_as_np[:,0].reshape((42000,1)) #To avoid having issues with arrays of np shape (42000,)\npixels = data_as_np[:,1:].reshape((42000,28,28,1)) / 255 # normalizing the inputs\n\nlabels, pixels = shuffle(labels, pixels)\n\nr = int(np.random.uniform(0,41999))\ndigit=pixels[r,:,:,0] * 255 #pyplot expects a 2D array for a gray scale image\nplt.imshow(digit, cmap=plt.cm.gray)   ","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fc4b9170790>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Split the data \n\nWe want to split the data we have into a training set and a crossing validation set. I will not use a test set in this application as we do not have a lot of data and we want to maximise the potential of what we have.\n\nI have taken a 80:20 split for this task.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_CV, y_train, y_CV = train_test_split(pixels, labels, test_size = 0.2 )\n\nprint(f'Training data: \\nX_train shape : {X_train.shape} and y_train shape : {y_train.shape}\\n')\nprint(f'Cross validation data: \\nX_CV shape : {X_CV.shape} and y_CV shape : {y_CV.shape}')","execution_count":6,"outputs":[{"output_type":"stream","text":"Training data: \nX_train shape : (33600, 28, 28, 1) and y_train shape : (33600, 1)\n\nCross validation data: \nX_CV shape : (8400, 28, 28, 1) and y_CV shape : (8400, 1)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Time to build the model v1\n\nFor this first version of my submission of the MNIST competition i will build a CNN similar to the LeNet5. Using tensorflow and Keras API."},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = keras.Input(shape = (28,28,1))\n\nconv1 = layers.Conv2D(6, (5,5), strides = (1,1), activation = 'tanh')\nx = conv1(inputs)\n\nx = layers.AveragePooling2D(pool_size=(2,2))(x)\nx = layers.Conv2D(16, (5,5), strides = (1,1), activation = 'tanh')(x)\nx = layers.AveragePooling2D(pool_size=(2,2))(x)\n\nx = layers.Flatten()(x)\nx = layers.Dense(120, activation = 'tanh')(x)\nx = layers.Dense(84, activation = 'tanh')(x)\n\noutputs = layers.Dense(10, activation = 'softmax')(x)\n\nmodel = keras.Model(inputs = inputs, outputs = outputs, name=\"MNISTv1\")\n\nmodel.summary()","execution_count":7,"outputs":[{"output_type":"stream","text":"Model: \"MNISTv1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 24, 24, 6)         156       \n_________________________________________________________________\naverage_pooling2d (AveragePo (None, 12, 12, 6)         0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 8, 8, 16)          2416      \n_________________________________________________________________\naverage_pooling2d_1 (Average (None, 4, 4, 16)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 256)               0         \n_________________________________________________________________\ndense (Dense)                (None, 120)               30840     \n_________________________________________________________________\ndense_1 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 44,426\nTrainable params: 44,426\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Compile the model\n\nNow I will compile the model and choose ADAM optimiztion algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n             optimizer = keras.optimizers.Adam(),\n             metrics = ['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n                    batch_size=32,\n                    epochs=5)","execution_count":8,"outputs":[{"output_type":"stream","text":"Train on 33600 samples\nEpoch 1/5\n33600/33600 [==============================] - 10s 292us/sample - loss: 0.3195 - accuracy: 0.9047\nEpoch 2/5\n33600/33600 [==============================] - 9s 277us/sample - loss: 0.1280 - accuracy: 0.9607\nEpoch 3/5\n33600/33600 [==============================] - 9s 274us/sample - loss: 0.0875 - accuracy: 0.9733\nEpoch 4/5\n33600/33600 [==============================] - 9s 274us/sample - loss: 0.0643 - accuracy: 0.9795\nEpoch 5/5\n33600/33600 [==============================] - 9s 278us/sample - loss: 0.0511 - accuracy: 0.9840\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Cross validate the model on an unknown set\n\nNow that our model is trained with a good accuracy on the training dataset (more than 98% with just 40k parameters!), we can check the values on unknown values and see the performances."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = model.evaluate(X_CV, y_CV, verbose=2)","execution_count":9,"outputs":[{"output_type":"stream","text":"8400/8400 - 1s - loss: 0.0768 - accuracy: 0.9783\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Remarks\n\nOur cross validation accuracy is slightly inferior to the train accuracy at 97.83% on my model. That means that the model is actually overfitting sightly the training set.\n\nPossible solutions to improve the overfiting (one or a combination of the following):\n\n- Add regularisation to the loss funciton\n- Add dropouts layers\n- Increase training data by means of data augmentation\n\n\nAlso, it is probably possible to train a bigger network on this data, then we would definitely need to use data augmentation techniques."},{"metadata":{},"cell_type":"markdown","source":"## Prediction\n\nFinally we need to make prediction on an unknown data using our model. We will then convert the results in a csv files for submission for the kaggle competition. \n\n### Inference times\n\nWe can see that the inference time for a 28 000 batch size is less than 4 seconds. This would go up if we did data augmentation at run time to enhance the performance or if we used several models to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\ntest_as_np = test.to_numpy()\n\npixels_test = test_as_np.reshape((28000,28,28,1)) / 255\n\ntic = time.time()\nresult = model.predict(pixels_test, batch_size=32)\ntoc = time.time()\n\nelapsed = (toc-tic)*1000\nprint(f'The inference time on the 28 000 batch size is {elapsed} ms')\n\n\nresult = result.argmax(axis=1).reshape((28000,1))","execution_count":73,"outputs":[{"output_type":"stream","text":"The inference time on the 28 000 batch size is 3898.545265197754 ms\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Prediction on a single example\n\nWe can compute the inference time on a single example and check if our model work properly."},{"metadata":{"trusted":true},"cell_type":"code","source":"i = int(np.random.uniform(0,27999))\n\ntic = time.time()\ninference = model.predict(pixels_test[i:i+1,:,:,:])\ntoc = time.time()\n\nelapsed = (toc-tic)*1000\n\nprint(f'The result is {inference.argmax(axis=1)} and the inference time to output that result is {elapsed} ms')\n\n#print(np.average(pixels_test[i:i+1,:,:,:]))\n#print('this is the shit \\n',np.average(pixels_test[i,:,:,0])) #Theses two lines can be used to make sure we are looking at the same example.\ndigit=pixels_test[i,:,:,0] * 255 \nplt.imshow(digit, cmap=plt.cm.gray)  ","execution_count":90,"outputs":[{"output_type":"stream","text":"0.16707182873149262\nThe result is [9] and the inference time to output that result is 31.569719314575195 ms\nthis is the shit \n 0.16707182873149262\n","name":"stdout"},{"output_type":"execute_result","execution_count":90,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fc4586f3710>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Postprocessing\n\nWe need to convert the results in a csv files for submission for the kaggle competition. The exact shape should have the id of the test exmaple and the class predcited."},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.arange(28000).reshape((28000,1))\ntest_result = np.concatenate((a, result), axis = 1)\n\nprint(test_result)","execution_count":47,"outputs":[{"output_type":"stream","text":"[[    0     2]\n [    1     0]\n [    2     9]\n ...\n [27997     3]\n [27998     9]\n [27999     2]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Save as a CSV file\n\nFinally, we have the data well ordered and everything is ready to create and submit the CSV file."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.savetxt('test.csv', test_result, fmt='%i', delimiter=',', header=\"#ImageId, #Label\")","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check = pd.read_csv('/kaggle/working/test.csv')\ncheck.head(10)","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"   # #ImageId   #Label\n0           0        2\n1           1        0\n2           2        9\n3           3        0\n4           4        3\n5           5        7\n6           6        0\n7           7        3\n8           8        0\n9           9        3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th># #ImageId</th>\n      <th>#Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}